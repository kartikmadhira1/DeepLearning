{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#premature optical flow setup to run on the movidius NCS\n",
    "\n",
    "\"\"\"\n",
    "Initial flow for the implementation of premature CNN optical flow supervised learning:\n",
    "1. Set up the tf network to resemble on of the API modes of tf that enables graph building(unlike the TF Estimator).\n",
    "    Something like the MNIST implementaion for NCS: https://github.com/HsinM/mnist-NCS \n",
    "2. Data Wrangling - Every input layer(a single pair) to the architecture is a 6 channel M*N data that needs to be\n",
    "    fed to the next layer. So the input pairs be formatted in the way (No.pairs,M,N,6)\n",
    "3. Add layers:\n",
    "    i. Conv and then relu activation - x4\n",
    "    ii. Deconv(which is not a dconv in any case but a transposed convolution) and then relu - x4\n",
    "4. Apply loss function as the vector subtraction on Forward pass and the P_actual\n",
    "5. After implementation:\n",
    "    1. Save model and check the names of input and output layers to be later used in the compilation\n",
    "    2. Compile using mvNCCompile and add input and output names in args\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are going to upload the pics and get them running!\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "im1=cv2.imread(\"data/00001_img1.ppm\")\n",
    "im2=cv2.imread(\"data/00001_img2.ppm\")\n",
    "imgStack=np.dstack((im1,im2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFlow(name):\n",
    "    if name.endswith('.pfm') or name.endswith('.PFM'):\n",
    "        return readPFM(name)[0][:,:,0:2]\n",
    "\n",
    "    f = open(name, 'rb')\n",
    "\n",
    "    header = f.read(4)\n",
    "    if header.decode(\"utf-8\") != 'PIEH':\n",
    "        raise Exception('Flow file header does not contain PIEH')\n",
    "\n",
    "    width = np.fromfile(f, np.int32, 1).squeeze()\n",
    "    height = np.fromfile(f, np.int32, 1).squeeze()\n",
    "\n",
    "    flow = np.fromfile(f, np.float32, width * height * 2).reshape((height, width, 2))\n",
    "\n",
    "    return flow.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=''\n",
    "readFlow(\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doubt - will pooling cause loss in information? and then unpooling\n",
    "#create the skeleton of the neural net\n",
    "def optNet(x_dict,n_classes,dropout,reuse,is_training):\n",
    "    #dont know of this step\n",
    "    with tf.variable_scope('optNet',reuse=reuse):\n",
    "        \"\"\"\n",
    "        this whole function is going to be used by a model_fn\n",
    "        which in turn would be passed to the tf class tf.estimator.Estimator\n",
    "        see more at \n",
    "        https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator\n",
    "        \"\"\"\n",
    "        #the input is going to be a dictionary\n",
    "        x=x_dict['images']\n",
    "        #first layer is Conv so reshape back to 2D form and get conv layer\n",
    "        #image=[batch size,height,width,channel]\n",
    "        x=tf.reshape(x,shape=[-1,28,28,1])\n",
    "        x = tf.identity(x, name='new_node')\n",
    "\n",
    "        #first layer is a 2D conv layer with 32 filters with a size of 5\n",
    "        #for a better understanding on what the number of filters mean: \n",
    "        #https://stackoverflow.com/questions/36243536/what-is-the-number-of-filter-in-cnn\n",
    "        conv1=tf.layers.conv2d(x,32,5,activation=tf.nn.relu)\n",
    "        #max pooling this 2D layer on down sampling(to get the max value only)\n",
    "        #the pooling has 2 step size in the stride and 2 kernel size\n",
    "        conv1=tf.layers.max_pooling2d(conv1,2,2)\n",
    "        \"\"\"\n",
    "        POOLING DOUBT!\n",
    "        \"\"\"\n",
    "        #same way we add another conv layer with 64 filters each of kernel 3\n",
    "        conv2=tf.layers.conv2d(conv1,64,3,activation=tf.nn.relu)\n",
    "        conv2=tf.layers.max_pooling2d(conv2,2,2)\n",
    "        \n",
    "        #continue till 4th layer\n",
    "        conv3=tf.layers.conv2d(conv2,128,5,activation=tf.nn.relu)\n",
    "        conv3=tf.layers.max_pooling2d(conv3,2,2)\n",
    "        \n",
    "        conv4=tf.layers.conv2d(conv1,512,7,activation=tf.nn.relu)\n",
    "        conv4=tf.layers.max_pooling2d(conv4,2,2)\n",
    "        \n",
    "        #deconv now\n",
    "        deconv1=tf.layers.conv2d_transpose(conv4,32,5,activation=tf.nn.relu)        \n",
    "        deconv2=tf.layers.conv2d(deconv1,32,3,activation=tf.nn.relu)        \n",
    "        deconv3=tf.layers.conv2d(deconv2,32,5,activation=tf.nn.relu)\n",
    "        deconv4=tf.layers.conv2d(deconv1,64,7,activation=tf.nn.relu)\n",
    "        \n",
    "        #flatten data now to get eventually get 10 classes of output\n",
    "        #fc1=tf.contrib.layers.flatten(conv2)\n",
    "        \n",
    "        #first connected layer consisting of 1024 neurons\n",
    "        #fc1=tf.layers.dense(fc1,1024)\n",
    "        #apply dropout when training(is_training is the bool)\n",
    "        #fc1=tf.layers.dropout(fc1,rate=dropout,training=is_training)\n",
    "        \n",
    "        #finally output just 10 classes required with a connected layer\n",
    "        #out=tf.layers.dense(fc1,n_classes,name='final')\n",
    "        out = tf.identity(deconv, name='last_node')\n",
    "    return out        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
