{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "definitions:\n",
    "d\n",
    "Training Parameter \n",
    "\n",
    "1. Learning Rate - The rate at which the Loss function gradient should descent\n",
    "2. Epoch - One forward and one backward pass of ALL training examples\n",
    "3. Batch size - The number of training examples to take in\n",
    "    one forward and backward pass or one BATCH' of data.\n",
    "4. Num of steps - lol no idea\n",
    "\n",
    "Network Parameters\n",
    "\n",
    "1. Input - The number of neurons in the first layer on in short the\n",
    "    unwrapped (2D to 1D) of an image = 28*28= 784 here\n",
    "2. Input classes - The number of classes the classification needs\n",
    "    to made for\n",
    "3. Dropout - For the model to NOT overfit the data, we tend to DROP some\n",
    "of the neurons and run the passes. It is the fraction to drop \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#training parameters\n",
    "learning_rate=0.01\n",
    "num_steps=2000\n",
    "batch_size=128\n",
    "\n",
    "#network params\n",
    "num_input=784\n",
    "num_classes=10\n",
    "dropout=0.25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create the skeleton of the neural net\n",
    "def conv_net(x_dict,n_classes,dropout,reuse,is_training):\n",
    "    #dont know of this step\n",
    "    with tf.variable_scope('ConvNet',reuse=reuse):\n",
    "        \"\"\"\n",
    "        this whole function is going to be used by a model_fn\n",
    "        which in turn would be passed to the tf class tf.estimator.Estimator\n",
    "        see more at \n",
    "        https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator\n",
    "        \"\"\"\n",
    "        #the input is going to be a dictionary\n",
    "        x=x_dict['images']\n",
    "        #first layer is Conv so reshape back to 2D form and get conv layer\n",
    "        #image=[batch size,height,width,channel]\n",
    "        x=tf.reshape(x,shape=[-1,28,28,1])\n",
    "        #first layer is a 2D conv layer with 32 filters with a size of 5\n",
    "        #for a better understanding on what the number of filters mean: \n",
    "        #https://stackoverflow.com/questions/36243536/what-is-the-number-of-filter-in-cnn\n",
    "        conv1=tf.layers.conv2d(x,32,5,activation=tf.nn.relu)\n",
    "        #max pooling this 2D layer on down sampling(to get the max value only)\n",
    "        #the pooling has 2 step size in the stride and 2 kernel size\n",
    "        conv1=tf.layers.max_pooling2d(conv1,2,2)\n",
    "        \n",
    "        #same way we add another conv layer with 64 filters each of kernel 3\n",
    "        conv2=tf.layers.conv2d(conv1,64,3,activation=tf.nn.relu)\n",
    "        conv2=tf.layers.max_pooling2d(conv2,2,2)\n",
    "        \n",
    "        #flatten data now to get eventually get 10 classes of output\n",
    "        fc1=tf.contrib.layers.flatten(conv2)\n",
    "        \n",
    "        #first connected layer consisting of 1024 neurons\n",
    "        fc1=tf.layers.dense(fc1,1024)\n",
    "        #apply dropout when training(is_training is the bool)\n",
    "        fc1=tf.layers.dropout(fc1,rate=dropout,training=is_training)\n",
    "        \n",
    "        #finally output just 10 classes required with a connected layer\n",
    "        out=tf.layers.dense(fc1,n_classes)\n",
    "    return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize a model function that is based on the template given at\n",
    "#https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator\n",
    "\n",
    "def model_fn(features,labels,mode):\n",
    "    \n",
    "    #building the NN, with two different graphs since the \n",
    "    #dropouts behave differently with training and predictions\n",
    "    mod_train=conv_net(features,num_classes,dropout,reuse=False,is_training=True)\n",
    "    mod_test=conv_net(features,num_classes,dropout,reuse=True,is_training=False)\n",
    "    \n",
    "    #softmax to convert prob between 0 to 1.\n",
    "    pred_classes=tf.argmax(mod_test,axis=1)\n",
    "    pred=tf.nn.softmax(mod_test)\n",
    "    \n",
    "     # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "    \n",
    "    #loss and optimizer\n",
    "    loss_fn=tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    logits=mod_train,labels=tf.cast(labels,dtype=tf.int32)))\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op=optimizer.minimize(loss_fn,global_step=tf.train.get_global_step())\n",
    "    \n",
    "    #get accuracy\n",
    "    accuracy=tf.metrics.accuracy(labels=labels,predictions=pred_classes)\n",
    "    \n",
    "    #TF estimator has to return an EstimatorSpec object (same in the case if\n",
    "    #mode =PREDICT)\n",
    "    \n",
    "    estimation_ret=tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=pred_classes,\n",
    "        loss=loss_fn,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops={'accuracy':accuracy})\n",
    "    \n",
    "    return estimation_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpXHee6f\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc349853050>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/tmpXHee6f', '_train_distribute': None, '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "model=tf.estimator.Estimator(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn=tf.estimator.inputs.numpy_input_fn(x={'images': mnist.train.images},y=mnist.train.labels,\n",
    "                                           batch_size=batch_size,num_epochs=None,shuffle=True)\n",
    "#start training the model\n",
    "#model.train(input_fn,steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
